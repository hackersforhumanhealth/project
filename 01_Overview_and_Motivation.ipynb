{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Using Social Media to Study the Link between Health and Happiness\n",
    "\n",
    "#### *Based on Twitter Data*\n",
    "\n",
    "\n",
    "##A Data Science CS109 Class Project\n",
    "\n",
    "Contributors\n",
    "-----\n",
    "In alphabetical order  \n",
    "\n",
    "Alejandro Covarrubias | Jacob Lurye | Eliud Oloo | Qiu-Yue Zhong\n",
    "\n",
    "\n",
    "#  \n",
    "<div style=\"float: right; margin-left: 30px;\"><img title=\"created by Stef Gibson at StefGibson.com\"style=\"float: right;margin-left: 30px;\" src=\"http://www.massage1.com/wp-content/uploads/healthhappiness.jpg\" align=right height = 350 /><figcaption>    image source: http://www.massage1.com/wp-content/uploads/healthhappiness</figcaption></div>\n",
    "\n",
    "\n",
    "#  \n",
    "\n",
    "###MOTIVATION\n",
    "------\n",
    "We are a team with diverse backgrounds in statistics, computer science, public health and biomedical research. The motivation for this project is our common interest in applying social media and data science approaches to study and promote human health. The scientific goal of this work was to come up with a way to measure average happiness state-by state using sentiment analysis of Twitter data and then to determine how well the happiness of a state correlates with public health statistics such as morbidity, mortality, healthcare quality and other criteria.\n",
    "\n",
    "\n",
    "\n",
    "###OVERVIEW\n",
    "------\n",
    "\n",
    "It is often stated that health and happiness are closely linked. Quantifiable evidence to that effect is however hard to come by.  One of many reasons for this that happiness is an ambiguous concept -- easy to recognize in oneself but often harder to detect, much less measure, in others.  Yet, if there was ever an opportune time to make a reasonable attempt at quantifying happiness in the population, it is now.  The rapid worldwide adoption of social media platforms in recent years has tremendously increased the amount, spontaneity and frequency of human communication.   Twitter usage alone grew six orders of magnitude from 5,000 tweets per day in 2007 to 500,000,000 tweets per day in 2013 http://www.internetlivestats.com/twitter-statistics/. And because the data is recorded electronically, it has another very attractive advantage over verbal communication for a data scientist - persistence.  It is remains available to be parsed and analyzed as new analysis methods and expertise become available long after the data was generated.  Studies have shown that self-disclosure in online communication is more frequent and revealing than in face-to-face communication presumably due to anonymity and physical distance https://en.wikipedia.org/wiki/Self-disclosure.  Together, all these factors translate into the availability of huge volumes of data to work with in trying to gauge a phenomenon as nebulous as happiness.  A number of researchers have taken advantag of these characteristics of mordern online communication and attempted to measure happiness using twitter data, for example http://hedonometer.org/index.html. We therefore leveraged the experiences published by other researches to inform our approach on how go about this project. Being able to reliably measure happiness opens the door to trying to find out what the determinants of happiness in the human population are. In particular we were interested in investigate whether public health outcomes released in various government and non-profit orgabnization reports have any relationship with happiness as measured on twitter.   \n",
    "\n",
    "See our project homepage [here](http://hackersforhumanhealth.me).\n",
    "\n",
    "\n",
    "###QUESTION\n",
    "------\n",
    "\n",
    "The main question we wanted to answer is: Do happier people generally experience better health and do healthier people experience more happiness?    To do this, we assumed that happiness as expressed in twitter communication is a surrogate for genuine happiness in the human population.  That of course is a big assumption but we felt that it is as good a starting point as you can get for investigating this very challenging problem.  Our first task was to obtain twitter data.  \n",
    "Since we wanted to learn how to harvest and process twitter data, we wrote a python script to do just that: <link to script>.  As a backup, we also were very fortunate to have access to a dataset of 3.5 million tweets generously provided by SÃ©bastien Gruhier of http://onemilliontweetmap.com/ , to whom we are very grateful. Due to twitter policy, we are unable to provide a public link to this data set.\n",
    "\n",
    "For our study, we chose to focus on the United States because of the limitation of time available for the project as well as the ready availability of both tweet and health data for the US.   For both data sources, we wrote separate scripts for  parsing and reformatting the data appropriately for analysis.  This involved selecting only US tweets, adding a US state of origin label to each tweet and cleaning out unwanted characters in the tweets.  \n",
    "Some of the challenges we encountered in processing our own harvested dataset are:\n",
    "\n",
    "the onemilliontweetmap dataset (hereafter referred to omtm) were:  huge file sized and insufficient memory to process in our laptop machines.  We worked around this by using the Unix grep command to select US tweets using search term \"United States\" and  dividing the data (text files into chuncks of a million lines (tweets) using the unix head - 1000000.     These smaller files were then separately processed using the script.\n",
    "The online Json Viewer resource http://jsonviewer.stack.hu/ proved to be a very useful tool for easily identifying the fields to extract using our script.   Similarly, the Json validator online tool http://jsonviewer.stack.hu/ was helpful in ascertaining that out output files were valid Json.  \n",
    "\n",
    "out script -- getting location data -- state and county using geopy geocoded, timed out, next time use solution proposed by \n",
    "gps good if you want o be more precise street level -- use soluti0n proposed by BF on piazza\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script for harvesting twitter data ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "ckey = ''\n",
    "csecret = ''\n",
    "atoken = ''\n",
    "asecret = ''\n",
    "\n",
    "oauth = OAuth(atoken, asecret, ckey, csecret)\n",
    "\n",
    "# Initiate the connection to Twitter Streaming API\n",
    "twitter_stream = TwitterStream(auth=oauth)\n",
    "\n",
    "# Get a sample of the public data following through Twitter\n",
    "iterator = twitter_stream.statuses.filter(locations='-126,-58,26,50', lang='en')\n",
    "\n",
    "print \"\\nHow many tweets would you like to collect?\"\n",
    "tweet_count = input()\n",
    "with open('tweets.csv','w') as tweet_file:\n",
    "\tfinal_dict = {'uid':[], 'tid':[], 'text':[], 'timestamp':[], 'city':[], 'country':[], 'bounding_box':[]}\n",
    "\tfor tweet in iterator:\n",
    "\t\ttweet_count -= 1\n",
    "\t\t# Twitter Python Tool wraps the data returned by Twitter \n",
    "\t\t# as a TwitterDictResponse object.\n",
    "\t\t# We convert it back to the JSON format to print/score\n",
    "\t\ttweet.values()\n",
    "\t\tfor k,v in tweet.iteritems():\n",
    "\t\t\tif k == 'text':\n",
    "\t\t\t\tfinal_dict['text'].append(v)\n",
    "\t\t\telif k == 'user':\n",
    "\t\t\t\tfinal_dict['uid'].append(v['id'])\n",
    "\t\t\telif k == 'id':\n",
    "\t\t\t\tfinal_dict['tid'].append(v)\n",
    "\t\t\telif k == 'timestamp_ms':\n",
    "\t\t\t\tfinal_dict['timestamp'].append(long(v))\n",
    "\t\t\telif k == 'place':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tfinal_dict['city'].append(v['full_name'].split(',')[0])\n",
    "\t\t\t\t\tfinal_dict['country'].append(v['country'])\n",
    "\t\t\t\t\tfinal_dict['bounding_box'].append(v['bounding_box'])\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tfinal_dict['city'].append('')\n",
    "\t\t\t\t\tfinal_dict['country'].append('')\n",
    "\t\t\t\t\tfinal_dict['bounding_box'].append('')\n",
    "\n",
    "\n",
    "\t\tif tweet_count <= 0:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\n",
    "\ttweet_df = pd.DataFrame(final_dict)\n",
    "\ttweet_df.to_csv(tweet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script for processing our own twitter dataset  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from geopy.geocoders import Nominatim\n",
    "import ast\n",
    "from time import sleep \n",
    "geolocator = Nominatim()\n",
    "\n",
    "\n",
    "'''\n",
    "input: twitter data file in CSV format\n",
    "intermediate step: used command line grep \"United States\" tweets.csv \n",
    "output: JSON file with United States tweets only, state label added and tweet text cleaned\n",
    "\n",
    "'''\n",
    " \n",
    " \n",
    "def find_tweet_address(gps_polygon_text):\n",
    "    \"\"\"\n",
    "    Get details about the location of origin of a tweet\n",
    "    based on GPS coordinates\n",
    "    \"\"\"\n",
    "    location_dict = None\n",
    "    gps_polygon_dict = ast.literal_eval(gps_polygon_text)\n",
    "    longitude =  gps_polygon_dict['coordinates'][0][0][0]\n",
    "    latitude =  gps_polygon_dict['coordinates'][0][0][1]\n",
    "    tweetlocation = geolocator.reverse((latitude, longitude), timeout=None)\n",
    "    tweetaddress_fields = (tweetlocation.raw)\n",
    "    try:\n",
    "        county = tweetaddress_fields['address']['county']\n",
    "        state = tweetaddress_fields['address']['state']\n",
    "        zipcode = tweetaddress_fields['address']['postcode']\n",
    "    except:\n",
    "        county = ''\n",
    "        state = ''\n",
    "        zipcode = ''\n",
    "    location_dict = dict(county=county, state=state, zipcode=zipcode)\n",
    "    return location_dict\n",
    " \n",
    "\n",
    "def tweet_cleaner(tweet):\n",
    "    \"\"\"\n",
    "    tweet cleaning function\n",
    "    adopted from http://ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/\n",
    "    \"\"\"\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "\n",
    " \n",
    "def parsecsv(tweet_data):\n",
    "    \"\"\"\n",
    "    parse each tweet and extract values of interest\n",
    "    \"\"\"\n",
    "    tweet_dict = None\n",
    "    if tweet_data[3] == \"United States\":\n",
    "        tweetid = tweet_data[-3]\n",
    "        userid = tweet_data[-1]\n",
    "        place = tweet_data[2]\n",
    "        coords = tweet_data[1]\n",
    "        country = tweet_data[3]\n",
    "        lang = ''\n",
    "        timestamp = tweet_data[-2]\n",
    "        ttext = tweet_data[4]\n",
    "        ttext_cleand = tweet_cleaner(ttext)\n",
    "        sleep(1)\n",
    "        location_data = find_tweet_address(coords)\n",
    "        state = location_data['state']\n",
    "        tweet_dict = dict(tweetid=tweetid, userid=userid, place=place, coords=coords, country=country, state=state, lang=lang,\n",
    "                         timestamp=timestamp, ttext=ttext, ttext_cleand=ttext_cleand)\n",
    "        # print \"\\n\", tweet_dict['ttext'], \"\\n\", tweet_dict['ttext_cleand'], \"\\n\", tweet_dict['state']\n",
    "    else:\n",
    "        pass\n",
    "    return tweet_dict\n",
    " \n",
    " \n",
    "def main():\n",
    "    \"\"\"\n",
    " \n",
    "    \"\"\"\n",
    "    line_count = 0\n",
    "    #open the file in universal-newline mode\n",
    "    with open(incsvfile, 'rU') as data_file:\n",
    "        data = csv.reader(data_file)\n",
    "        print ('[')\n",
    "        for tweet_data in data:\n",
    "            tweet_dict = parsecsv(tweet_data)\n",
    "            out_put = json.dumps(tweet_dict)\n",
    "            if out_put != 'null':\n",
    "                if line_count == 0:\n",
    "                    print (out_put)\n",
    "                else:\n",
    "                    print (\",\" + out_put)\n",
    "                line_count = 1\n",
    "        print (']')\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    incsvfile = sys.argv[1]\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "script for processinf omtm twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#import regex\n",
    "import re\n",
    "\n",
    "'''\n",
    "input: twitter data file in JSON format\n",
    "output: JSON file with United States tweets only, state label added and tweet text cleaned\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#start process_tweet\n",
    "def tweetcleaner(tweet):\n",
    "    ''' function adopted from http://ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/'''\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "#end\n",
    "\n",
    "\n",
    "def parsejson(tweet_data):\n",
    "    tweet_dict = None\n",
    "    if tweet_data[\"_source\"][\"place\"][\"country_code\"] == \"US\":\n",
    "        tweetid = tweet_data[\"_id\"]\n",
    "        userid = tweet_data[\"_source\"][\"user\"][\"id\"]\n",
    "        place = tweet_data[\"_source\"][\"place\"][\"full_name\"]\n",
    "        coords = tweet_data[\"_source\"][\"coordinates\"]\n",
    "        country = tweet_data[\"_source\"][\"place\"][\"country\"]\n",
    "        lang = tweet_data[\"_source\"][\"lang\"]\n",
    "        timestamp = tweet_data[\"_source\"][\"timestamp_ms\"]\n",
    "        ttext = tweet_data[\"_source\"][\"text\"]\n",
    "        ttext_cleand = tweetcleaner(ttext)\n",
    "        state = place.strip()[-3:].strip()\n",
    "        if state == 'USA':\n",
    "            state = place.split(\",\")[0]\n",
    "        else:\n",
    "            state = state\n",
    "            \n",
    "        tweet_dict = dict(tweetid=tweetid, userid=userid, place=place, coords=coords, country=country, state=state, lang=lang,\n",
    "                         timestamp=timestamp, ttext=ttext, ttext_cleand=ttext_cleand)\n",
    "        #print \"\\n\", tweetdict['ttext'], \"\\n\", tweetdict['ttext_cleand'], \"\\n\", tweetdict['state']\n",
    "    else:\n",
    "        pass\n",
    "    return tweet_dict\n",
    " \n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    line_count = 0\n",
    "    with open(injsonfile) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    with open(outjsonfile, 'w') as fp:\n",
    "        fp.write('[' + '\\n')\n",
    "        for tweet_data in data:\n",
    "            tweet_dict = parsejson(tweet_data)\n",
    "            out_put = json.dumps(tweet_dict)\n",
    "            if out_put != 'null':\n",
    "                if line_count == 0:\n",
    "                    fp.write(out_put + '\\n')\n",
    "                else:\n",
    "                    fp.write(\",\" + out_put + '\\n')\n",
    "                line_count = 1\n",
    "        fp.write(']' + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    injsonfile = raw_input(\"what is your input json file name? \")\n",
    "    outjsonfile = raw_input(\"what is your output json file name? \")\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
