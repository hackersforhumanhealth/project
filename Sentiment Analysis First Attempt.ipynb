{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "\n",
    "#Natural Language Toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load in the DF of tweets\n",
    "tweet_df = pd.read_csv('tweets1.csv', index_col=0, encoding='utf-8')\n",
    "\n",
    "#Consider only tweets from the US\n",
    "tweet_df = tweet_df[tweet_df.country == 'United States']\n",
    "\n",
    "#Load in the dataframe of word-happiness rankings\n",
    "sent_df = pd.DataFrame.from_csv('Data_Set_S1.txt', sep='\\t', index_col=None)\n",
    "\n",
    "#Load in stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare some regular expressions to handle abbreviations that people sometimes extend for emphasis like 'lollllll' or 'lmaoooo' or 'hahaha'. (Haven't finished this yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#lollll -> lol. lmaoooooo -> lmao.\n",
    "emph_stem = lambda s: re.sub(r'((\\w)\\2{2,})','\\\\2', s)\n",
    "\n",
    "#this has it's own happiness rating\n",
    "lolol = lambda s: re.sub(r'lolol(ol)*', 'lolol', s)\n",
    "\n",
    "#(all three of these appear separately in the word positivity list)\n",
    "haha = lambda s: re.sub(r'[a|h]{3,4}','haha', s)\n",
    "hahaha = lambda s: re.sub(r'[a|h]{5,6}','hahaha', s)\n",
    "hahahaha = lambda s: re.sub(r'[a|h]{7,140}','hahahaha', s)\n",
    "\n",
    "def apply_re(word):\n",
    "    if lolol(word) != word:\n",
    "        return lolol(word)\n",
    "    elif haha(word) != word:\n",
    "        return haha(word)\n",
    "    elif hahaha(word) != word:\n",
    "        return hahaha(word)\n",
    "    elif hahahaha(word) != word:\n",
    "        return hahahaha(word)\n",
    "    elif emph_stem(word) != word:\n",
    "        return emph_stem(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Let's create some functions to get simple happiness metrics for each tweet.\n",
    "\n",
    "Here's one that takes in raw tweet text, removes stopwords and punctuation (this includes emoticons, but maybe it shouldn't), and returns the tweet as a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tweet_to_list(tweet):\n",
    "    \n",
    "    tweet_list = nltk.tokenize.word_tokenize(tweet)\n",
    "    tweet_list = [word.lower() for word in tweet_list if word.isalpha()]\n",
    "    \n",
    "    cachedStopWords.append('https')\n",
    "    tweet_list = [word for word in tweet_list if word not in cachedStopWords]\n",
    "    tweet_list = [apply_re(word) for word in tweet_list]\n",
    "    \n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function avg_happiness takes in the list of words in a tweet, obtains the frequency distribution of each word, then calculates a weighted average \"happiness\" score based on each word's happiness rating (from Data_Set_S1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_happiness(text_list):\n",
    "    freqdist = nltk.FreqDist(text_list).items()\n",
    "    \n",
    "    happ_sum = 0\n",
    "    count = 0\n",
    "    \n",
    "    for i in freqdist:\n",
    "        word_row = sent_df[sent_df.word == i[0]]\n",
    "        happ_score = word_row.happiness_average.values\n",
    "        for i in happ_score:\n",
    "            happ_sum += int(i)\n",
    "            count += 1\n",
    "            \n",
    "    if count == 0:\n",
    "        return None\n",
    "    \n",
    "    return float(happ_sum)/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a new row in tweet_df with happiness scores for each tweet. Note that with only around 35,000 tweets, the following takes ~30 minutes to run. This means that we probably shouldn't use all of the million+ tweets we're collecting â€” rather, we should probably try to sample a reasonable, equal number of tweets from each state in the hopes of obtaining balanced representation (i.e., excluding some tweets from the more twitter-heavy states once we have enough overall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "happiness_score = lambda tweet: avg_happiness(tweet_to_list(tweet))\n",
    "\n",
    "happ_scores = tweet_df.text.apply(happiness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add column of happiness scores\n",
    "tweet_df['happiness'] = pd.Series(happ_scores)\n",
    "\n",
    "#remove tweets with NaN happiness scores\n",
    "tweet_df.dropna(inplace=True)\n",
    "\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "city_list = ['New York', 'Detroit', 'Chicago']\n",
    "\n",
    "for city in city_list:\n",
    "    happ_scores = tweet_df.happiness[tweet_df.city==city]\n",
    "    ax = sns.kdeplot(happ_scores, label=city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def norm(col):\n",
    "    return (col - np.mean(col))/(max(col) - min(col))\n",
    "\n",
    "tweet_df.norm_happ = norm(tweet_df.happiness)\n",
    "\n",
    "#add boolean tweet positivity indicator\n",
    "tweet_df['bool_happ'] = pd.Series(tweet_df.norm_happ > 0)\n",
    "\n",
    "print 'Ratio positive tweets in New York: ' + str(np.mean(tweet_df[tweet_df.city=='New York'].bool_happ))\n",
    "print 'Ratio positive tweets in Arizona: ' + str(np.mean(tweet_df[tweet_df.city=='Arizona'].bool_happ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df.to_csv('tweets_w_sent.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
