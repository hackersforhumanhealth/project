{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METHOD 1: Using Pandas  Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bounding_box</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>text</th>\n",
       "      <th>tid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-63.3...</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>\"Si empesamos hablar se pudre con la doli..\"</td>\n",
       "      <td>673735583940878338</td>\n",
       "      <td>1449466055270</td>\n",
       "      <td>2564256692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-45.1...</td>\n",
       "      <td>Cruzeiro</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>SÃ£o Paulo</td>\n",
       "      <td>\"que paia stalkear a pessoa e curtir a foto se...</td>\n",
       "      <td>673735583693451264</td>\n",
       "      <td>1449466055211</td>\n",
       "      <td>2237980915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-73.9...</td>\n",
       "      <td>Fort Lee</td>\n",
       "      <td>United States</td>\n",
       "      <td>NJ</td>\n",
       "      <td>\"Hmuuuu\"</td>\n",
       "      <td>673735584129671168</td>\n",
       "      <td>1449466055315</td>\n",
       "      <td>334470631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-80.2...</td>\n",
       "      <td>Boca Raton</td>\n",
       "      <td>United States</td>\n",
       "      <td>FL</td>\n",
       "      <td>\"Se respira el aire de cambio. #elcambiovieney...</td>\n",
       "      <td>673735584096063488</td>\n",
       "      <td>1449466055307</td>\n",
       "      <td>69791277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-73.3...</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>\"Y va a caer, y va a caer.. este gobierno va a...</td>\n",
       "      <td>673735584549101568</td>\n",
       "      <td>1449466055415</td>\n",
       "      <td>201410531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        bounding_box          city        country       state                                               text                 tid      timestamp         uid\n",
       "0  {u'type': u'Polygon', u'coordinates': [[[-63.3...  Buenos Aires      Argentina   Argentina       \"Si empesamos hablar se pudre con la doli..\"  673735583940878338  1449466055270  2564256692\n",
       "1  {u'type': u'Polygon', u'coordinates': [[[-45.1...      Cruzeiro         Brasil   SÃ£o Paulo  \"que paia stalkear a pessoa e curtir a foto se...  673735583693451264  1449466055211  2237980915\n",
       "2  {u'type': u'Polygon', u'coordinates': [[[-73.9...      Fort Lee  United States          NJ                                           \"Hmuuuu\"  673735584129671168  1449466055315   334470631\n",
       "3  {u'type': u'Polygon', u'coordinates': [[[-80.2...    Boca Raton  United States          FL  \"Se respira el aire de cambio. #elcambiovieney...  673735584096063488  1449466055307    69791277\n",
       "4  {u'type': u'Polygon', u'coordinates': [[[-73.3...     Venezuela      Venezuela   Venezuela  \"Y va a caer, y va a caer.. este gobierno va a...  673735584549101568  1449466055415   201410531"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# METHOD 1: convert csv file to pandas dataframe\n",
    "colnames=['bounding_box','city','country','state', 'text','tid','timestamp','uid']\n",
    "pandas_df = pd.read_csv('./newtweets_20151207_1228.csv', delimiter=',', skiprows=1, names=colnames, usecols=[1,2,3,4,5,6,7,8])\n",
    "pandas_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Argentina\n",
       "1           Brasil\n",
       "2    United States\n",
       "3    United States\n",
       "4        Venezuela\n",
       "Name: country, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bounding_box</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>text</th>\n",
       "      <th>tid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-73.9...</td>\n",
       "      <td>Fort Lee</td>\n",
       "      <td>United States</td>\n",
       "      <td>NJ</td>\n",
       "      <td>\"Hmuuuu\"</td>\n",
       "      <td>673735584129671168</td>\n",
       "      <td>1449466055315</td>\n",
       "      <td>334470631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-80.2...</td>\n",
       "      <td>Boca Raton</td>\n",
       "      <td>United States</td>\n",
       "      <td>FL</td>\n",
       "      <td>\"Se respira el aire de cambio. #elcambiovieney...</td>\n",
       "      <td>673735584096063488</td>\n",
       "      <td>1449466055307</td>\n",
       "      <td>69791277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-119....</td>\n",
       "      <td>Farmersville</td>\n",
       "      <td>United States</td>\n",
       "      <td>CA</td>\n",
       "      <td>\"@fcornejo9 they're this week???\"</td>\n",
       "      <td>673735584238559233</td>\n",
       "      <td>1449466055341</td>\n",
       "      <td>572075963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-85.6...</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>\"Squad goals with bae. ðŸ˜Š https://t.co/TBKgqhQ...</td>\n",
       "      <td>673735584142114822</td>\n",
       "      <td>1449466055318</td>\n",
       "      <td>1602156134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-84.3...</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>United States</td>\n",
       "      <td>KY</td>\n",
       "      <td>\"my girl I miss you so much and all of the tac...</td>\n",
       "      <td>673735584880459777</td>\n",
       "      <td>1449466055494</td>\n",
       "      <td>50910424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        bounding_box          city        country state                                               text                 tid      timestamp         uid\n",
       "2  {u'type': u'Polygon', u'coordinates': [[[-73.9...      Fort Lee  United States    NJ                                           \"Hmuuuu\"  673735584129671168  1449466055315   334470631\n",
       "3  {u'type': u'Polygon', u'coordinates': [[[-80.2...    Boca Raton  United States    FL  \"Se respira el aire de cambio. #elcambiovieney...  673735584096063488  1449466055307    69791277\n",
       "5  {u'type': u'Polygon', u'coordinates': [[[-119....  Farmersville  United States    CA                  \"@fcornejo9 they're this week???\"  673735584238559233  1449466055341   572075963\n",
       "7  {u'type': u'Polygon', u'coordinates': [[[-85.6...       Georgia  United States   USA  \"Squad goals with bae. ðŸ˜Š https://t.co/TBKgqhQ...  673735584142114822  1449466055318  1602156134\n",
       "9  {u'type': u'Polygon', u'coordinates': [[[-84.3...      Richmond  United States    KY  \"my girl I miss you so much and all of the tac...  673735584880459777  1449466055494    50910424"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usdf = pandas_df[pandas_df.country == \"United States\"]\n",
    "usdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract address info from gps coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'display_name': u'Dolphin Point Trail Southwest, Vashon, King County, Washington, 98070, United States of America', u'place_id': u'47783845', u'lon': u'-122.4558659', u'osm_type': u'way', u'licence': u'Data \\xa9 OpenStreetMap contributors, ODbL 1.0. http://www.openstreetmap.org/copyright', u'osm_id': u'6432047', u'lat': u'47.501025', u'address': {u'footway': u'Dolphin Point Trail Southwest', u'locality': u'Vashon', u'country': u'United States of America', u'county': u'King County', u'state': u'Washington', u'postcode': u'98070', u'country_code': u'us'}}\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "latitude = 47.495315\n",
    "longitude = -122.436232\n",
    "tweetloc = geolocator.reverse((latitude, longitude))\n",
    "tweetaddr = (tweetloc.raw)\n",
    "print tweetaddr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_tweet_address_v0(gps_polygon_text):\n",
    "    location_dict = None\n",
    "    gps_polygon_dict = ast.literal_eval(gps_polygon_text)\n",
    "    longitude =  gps_polygon_dict['coordinates'][0][0][0]\n",
    "    latitude =  gps_polygon_dict['coordinates'][0][0][1]\n",
    "    tweetlocation = geolocator.reverse((latitude, longitude))\n",
    "    tweetaddress_fields = (tweetlocation.raw)\n",
    "    try:\n",
    "        county = tweetaddress_fields['address']['county']\n",
    "        state = tweetaddress_fields['address']['state']\n",
    "        zipcode = tweetaddress_fields['address']['postcode']\n",
    "    except:\n",
    "        county = ''\n",
    "        state = ''\n",
    "        zipcode = ''      \n",
    "    location_dict = dict(county=county, state=state, zipcode=zipcode)\n",
    "    return zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_tweet_address_v1(gps_polygon_text):\n",
    "    gps_polygon_dict = ast.literal_eval(gps_polygon_text)\n",
    "    longitude =  gps_polygon_dict['coordinates'][0][0][0]\n",
    "    latitude =  gps_polygon_dict['coordinates'][0][0][1]\n",
    "    tweetlocation = geolocator.reverse((latitude, longitude))\n",
    "    tweetaddress_fields = (tweetlocation.raw)\n",
    "    try:\n",
    "        #county = tweetaddress_fields['address']['county']\n",
    "        #state = tweetaddress_fields['address']['state']\n",
    "        zipcode = tweetaddress_fields['address']['postcode']\n",
    "    except:\n",
    "\n",
    "        #county = ''\n",
    "        #state = ''\n",
    "        zipcode = ''      \n",
    "    return zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 620 ms, sys: 33.5 ms, total: 654 ms\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "usdf = pandas_df[pandas_df.country == \"United States\"].head(50)\n",
    "usdf['zipcode'] = usdf.bounding_box.apply(lambda x: find_tweet_address_v1(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bounding_box</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>text</th>\n",
       "      <th>tid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>uid</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-73.9...</td>\n",
       "      <td>Fort Lee</td>\n",
       "      <td>United States</td>\n",
       "      <td>NJ</td>\n",
       "      <td>\"Hmuuuu\"</td>\n",
       "      <td>673735584129671168</td>\n",
       "      <td>1449466055315</td>\n",
       "      <td>334470631</td>\n",
       "      <td>07010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-80.2...</td>\n",
       "      <td>Boca Raton</td>\n",
       "      <td>United States</td>\n",
       "      <td>FL</td>\n",
       "      <td>\"Se respira el aire de cambio. #elcambiovieney...</td>\n",
       "      <td>673735584096063488</td>\n",
       "      <td>1449466055307</td>\n",
       "      <td>69791277</td>\n",
       "      <td>33076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-119....</td>\n",
       "      <td>Farmersville</td>\n",
       "      <td>United States</td>\n",
       "      <td>CA</td>\n",
       "      <td>\"@fcornejo9 they're this week???\"</td>\n",
       "      <td>673735584238559233</td>\n",
       "      <td>1449466055341</td>\n",
       "      <td>572075963</td>\n",
       "      <td>93292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-85.6...</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>\"Squad goals with bae. ðŸ˜Š https://t.co/TBKgqhQ...</td>\n",
       "      <td>673735584142114822</td>\n",
       "      <td>1449466055318</td>\n",
       "      <td>1602156134</td>\n",
       "      <td>32409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{u'type': u'Polygon', u'coordinates': [[[-84.3...</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>United States</td>\n",
       "      <td>KY</td>\n",
       "      <td>\"my girl I miss you so much and all of the tac...</td>\n",
       "      <td>673735584880459777</td>\n",
       "      <td>1449466055494</td>\n",
       "      <td>50910424</td>\n",
       "      <td>40475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        bounding_box          city        country state                                               text                 tid      timestamp         uid zipcode\n",
       "2  {u'type': u'Polygon', u'coordinates': [[[-73.9...      Fort Lee  United States    NJ                                           \"Hmuuuu\"  673735584129671168  1449466055315   334470631   07010\n",
       "3  {u'type': u'Polygon', u'coordinates': [[[-80.2...    Boca Raton  United States    FL  \"Se respira el aire de cambio. #elcambiovieney...  673735584096063488  1449466055307    69791277   33076\n",
       "5  {u'type': u'Polygon', u'coordinates': [[[-119....  Farmersville  United States    CA                  \"@fcornejo9 they're this week???\"  673735584238559233  1449466055341   572075963   93292\n",
       "7  {u'type': u'Polygon', u'coordinates': [[[-85.6...       Georgia  United States   USA  \"Squad goals with bae. ðŸ˜Š https://t.co/TBKgqhQ...  673735584142114822  1449466055318  1602156134   32409\n",
       "9  {u'type': u'Polygon', u'coordinates': [[[-84.3...      Richmond  United States    KY  \"my girl I miss you so much and all of the tac...  673735584880459777  1449466055494    50910424   40475"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##METHOD 2: Using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/apache-spark/libexec\n"
     ]
    }
   ],
   "source": [
    "#create spark dataframe\n",
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"4g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "rdd.map(lambda x: sys.version).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets:  37519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[u'0',\n",
       "  u'\"{u\\'type\\': u\\'Polygon\\'',\n",
       "  u\" u'coordinates': [[[-118.082615\",\n",
       "  u' 33.628991]',\n",
       "  u' [-118.082615',\n",
       "  u' 33.756093]',\n",
       "  u' [-117.91485',\n",
       "  u' 33.756093]',\n",
       "  u' [-117.91485',\n",
       "  u' 33.628991]]]}\"',\n",
       "  u'Huntington Beach',\n",
       "  u'United States',\n",
       "  u'In need',\n",
       "  u' of back rub',\n",
       "  u'669375138903666692',\n",
       "  u'1448426444160',\n",
       "  u'1611125173'],\n",
       " [u'1',\n",
       "  u'\"{u\\'type\\': u\\'Polygon\\'',\n",
       "  u\" u'coordinates': [[[-82.87387\",\n",
       "  u' 38.688052]',\n",
       "  u' [-82.87387',\n",
       "  u' 38.76256]',\n",
       "  u' [-82.811927',\n",
       "  u' 38.76256]',\n",
       "  u' [-82.811927',\n",
       "  u' 38.688052]]]}\"',\n",
       "  u'Wheelersburg',\n",
       "  u'United States',\n",
       "  u\"And I'm always tired but never of you\",\n",
       "  u'669375138958307328',\n",
       "  u'1448426444173',\n",
       "  u'3165522051']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv to spark RDD then creat spark dataframe from it\n",
    "alltweets_rdd = sc.textFile('./tweets.csv')\n",
    "alltweets_rdd = alltweets_rdd.map(lambda line: line.split(\",\"))\n",
    "print \"Total number of tweets: \", alltweets_rdd.count()\n",
    "alltweets_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create spark dataframe from pandas dataframe of csv file\n",
    "#alltweets_df = sqlsc.createDataFrame(pandas_df)\n",
    "#alltweets_df.printSchema()\n",
    "#alltweets_df.show(5)\n",
    "#ustweets_df = alltweets_df[alltweets_df['country'] == \"United States\"]\n",
    "#ustweets_df = alltweets_df.filter(alltweets_df['country'] == \"United States\")\n",
    "#print \"Count of All tweets =\", alltweets_df.count()\n",
    "#print \"Count of US tweets:\", ustweets_df.count()\n",
    "#ustweets_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bounding_box: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tid: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      "\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "37519\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#create spark dataframe from rdd of csv file\n",
    "alltweets_rdd = sc.textFile('./tweets.csv')\n",
    "alltweets_rdd = alltweets_rdd.map(lambda line: line.split(\",\"))\n",
    "header = alltweets_rdd.first()\n",
    "#alltweets_rdd = alltweets_rdd.filter(lambda line:line != header)\n",
    "#alltweets_df0 = alltweets_rdd.map(lambda line: Row(bounding_box=line[1:10], city=line[10], country=line[11], \n",
    "#                              text=line[12:-3], tid=line[-3], timestamp=line[-2], uid=line[-1])).toDF()\n",
    "alltweets_df0 = alltweets_rdd.map(lambda line: Row(bounding_box=line[1:10], city=line[10], country=line[11], \n",
    "                              text=line[12:-3], tid=line[-3], timestamp=line[-2], uid=line[-1]))\n",
    "alltweets_df = sqlsc.createDataFrame(alltweets_df0)\n",
    "#alltweets_df.registerTempTable(\"alltweets_df\")\n",
    "alltweets_df.printSchema()\n",
    "\n",
    "print type(alltweets_rdd)\n",
    "print alltweets_rdd.count()\n",
    "print type(alltweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RDD actions like count() or take() do not seem to work \n",
    "# for the alltweets_df object. Is it because dataset is too big?\n",
    "# Not sure why the following return an IndexError: \"list index out of range\"  \n",
    "#alltweets_df.count()\n",
    "#alltweets_df.take(2)\n",
    "#alltweets_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(bounding_box=[u'\"{u\\'type\\': u\\'Polygon\\'', u\" u'coordinates': [[[-118.082615\", u' 33.628991]', u' [-118.082615', u' 33.756093]', u' [-117.91485', u' 33.756093]', u' [-117.91485', u' 33.628991]]]}\"'], city=u'Huntington Beach', country=u'United States', text=[u'In need', u' of back rub'], tid=u'669375138903666692', timestamp=u'1448426444160', uid=u'1611125173')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alltweets_df.head()\n",
    "alltweets_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text=[u'In need', u' of back rub'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alltweets_df.select(\"text\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_states = alltweets_df.map(lambda r: find_tweet_address_v2(r.bounding_box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 17, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-15-b5c86b140645>\", line 8, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:119)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:111)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:111)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1850)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-15-b5c86b140645>\", line 8, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:119)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:111)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:111)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e9dc45f02ceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweet_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 17, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-15-b5c86b140645>\", line 8, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:119)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:111)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:111)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1850)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-15-b5c86b140645>\", line 8, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:119)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:111)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:111)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\n"
     ]
    }
   ],
   "source": [
    "tweet_states.take(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
