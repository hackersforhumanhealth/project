{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/apache-spark/libexec\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()\n",
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"4g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check child execution processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]',\n",
       " '2.7.10 |Anaconda 2.3.0 (x86_64)| (default, Oct 19 2015, 18:31:17) \\n[GCC 4.2.1 (Apple Inc. build 5577)]']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "rdd.map(lambda x: sys.version).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###SQL context for data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bounding_box: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- tid: string (nullable = true)\n",
      " |-- timestamp: double (nullable = true)\n",
      " |-- uid: double (nullable = true)\n",
      "\n",
      "+--------------------+----------------+-------------+--------------------+------------------+--------------------+----------------+\n",
      "|        bounding_box|            city|      country|                text|               tid|           timestamp|             uid|\n",
      "+--------------------+----------------+-------------+--------------------+------------------+--------------------+----------------+\n",
      "|{u'type': u'Polyg...|Huntington Beach|United States|             In need|       of back rub|6.693751389036666...|1.44842644416E12|\n",
      "|{u'type': u'Polyg...|    Wheelersburg|United States|And I'm always ti...|669375138958307328|   1.448426444173E12|   3.165522051E9|\n",
      "|{u'type': u'Polyg...|        Cascavel|       Brasil|    @DudaDalmas eita|669375138555633664|   1.448426444077E12|    1.71176445E8|\n",
      "|{u'type': u'Polyg...|        Rubidoux|United States|Lugo: I want to o...|669375137716637696|   1.448426443877E12|   2.260612922E9|\n",
      "|{u'type': u'Polyg...|           Ahome|      MÃ©xico|@marifer_robles s...|669375138341584899|   1.448426444026E12|    2.09377429E8|\n",
      "+--------------------+----------------+-------------+--------------------+------------------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Count of All tweets = 35473\n",
      "Count of US tweets: 21808\n",
      "+--------------------+----------------+-------------+--------------------+------------------+--------------------+----------------+\n",
      "|        bounding_box|            city|      country|                text|               tid|           timestamp|             uid|\n",
      "+--------------------+----------------+-------------+--------------------+------------------+--------------------+----------------+\n",
      "|{u'type': u'Polyg...|Huntington Beach|United States|             In need|       of back rub|6.693751389036666...|1.44842644416E12|\n",
      "|{u'type': u'Polyg...|    Wheelersburg|United States|And I'm always ti...|669375138958307328|   1.448426444173E12|   3.165522051E9|\n",
      "|{u'type': u'Polyg...|        Rubidoux|United States|Lugo: I want to o...|669375137716637696|   1.448426443877E12|   2.260612922E9|\n",
      "|{u'type': u'Polyg...|    Murfreesboro|United States|@_Shawncameron sl...|669375139050627072|   1.448426444195E12|    1.58503514E8|\n",
      "|{u'type': u'Polyg...|        Michigan|United States|@devilchasnme @Sh...|669375139172261888|   1.448426444224E12|     2.2429865E7|\n",
      "+--------------------+----------------+-------------+--------------------+------------------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    United States\n",
       "1    United States\n",
       "2           Brasil\n",
       "3    United States\n",
       "4           México\n",
       "Name: country, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'''\n",
    "# METHOD 1: convert csv file to pandas dataframe then read ino spark\n",
    "colnames=['bounding_box','city','country','text','tid','timestamp','uid']\n",
    "pandas_df = pd.read_csv('./tweets.csv', delimiter=',', skiprows=0, names=colnames, usecols=[1,2,3,4,5,6,7])\n",
    "pandas_df.head(5)\n",
    "alltweets_df = sqlsc.createDataFrame(pandas_df)\n",
    "alltweets_df.printSchema()\n",
    "alltweets_df.show(5)\n",
    "#ustweets_df = alltweets_df[alltweets_df['country'] == \"United States\"]\n",
    "ustweets_df = alltweets_df.filter(alltweets_df['country'] == \"United States\")\n",
    "print \"Count of All tweets =\", alltweets_df.count()\n",
    "print \"Count of US tweets:\", ustweets_df.count()\n",
    "ustweets_df.show(5)\n",
    "pandas_df.country.head()\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets:  37519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[u'0',\n",
       "  u'\"{u\\'type\\': u\\'Polygon\\'',\n",
       "  u\" u'coordinates': [[[-118.082615\",\n",
       "  u' 33.628991]',\n",
       "  u' [-118.082615',\n",
       "  u' 33.756093]',\n",
       "  u' [-117.91485',\n",
       "  u' 33.756093]',\n",
       "  u' [-117.91485',\n",
       "  u' 33.628991]]]}\"',\n",
       "  u'Huntington Beach',\n",
       "  u'United States',\n",
       "  u'In need',\n",
       "  u' of back rub',\n",
       "  u'669375138903666692',\n",
       "  u'1448426444160',\n",
       "  u'1611125173'],\n",
       " [u'1',\n",
       "  u'\"{u\\'type\\': u\\'Polygon\\'',\n",
       "  u\" u'coordinates': [[[-82.87387\",\n",
       "  u' 38.688052]',\n",
       "  u' [-82.87387',\n",
       "  u' 38.76256]',\n",
       "  u' [-82.811927',\n",
       "  u' 38.76256]',\n",
       "  u' [-82.811927',\n",
       "  u' 38.688052]]]}\"',\n",
       "  u'Wheelersburg',\n",
       "  u'United States',\n",
       "  u\"And I'm always tired but never of you\",\n",
       "  u'669375138958307328',\n",
       "  u'1448426444173',\n",
       "  u'3165522051']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# METHOD 2: read csv directly to spark RDD then creat spark dataframe from it\n",
    "alltweets_rdd = sc.textFile('./tweets.csv')\n",
    "alltweets_rdd = alltweets_rdd.map(lambda line: line.split(\",\"))\n",
    "print \"Total number of tweets: \", alltweets_rdd.count()\n",
    "alltweets_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bounding_box: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tid: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      "\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "37519\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "alltweets_rdd = sc.textFile('./tweets.csv')\n",
    "alltweets_rdd = alltweets_rdd.map(lambda line: line.split(\",\"))\n",
    "header = alltweets_rdd.first()\n",
    "#alltweets_rdd = alltweets_rdd.filter(lambda line:line != header)\n",
    "#alltweets_df = alltweets_rdd.map(lambda line: Row(bounding_box=line[1:10], city=line[10], country=line[11], \n",
    "#                              text=line[12:-3], tid=line[-3], timestamp=line[-2], uid=line[-1])).toDF()\n",
    "alltweets_df0 = alltweets_rdd.map(lambda line: Row(bounding_box=line[1:10], city=line[10], country=line[11], \n",
    "                              text=line[12:-3], tid=line[-3], timestamp=line[-2], uid=line[-1]))\n",
    "alltweets_df = sqlsc.createDataFrame(alltweets_df0)\n",
    "#alltweets_df.registerTempTable(\"alltweets_df\")\n",
    "alltweets_df.printSchema()\n",
    "\n",
    "print type(alltweets_rdd)\n",
    "print alltweets_rdd.count()\n",
    "print type(alltweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RDD actions like count() or take() do not seem to work \n",
    "# for the alltweets_df object. Is it because dataset is too big?\n",
    "# Not sure why the following return an IndexError: \"list index out of range\"  \n",
    "#alltweets_df.count()\n",
    "#alltweets_df.take(2)\n",
    "#alltweets_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(bounding_box=[u'\"{u\\'type\\': u\\'Polygon\\'', u\" u'coordinates': [[[-118.082615\", u' 33.628991]', u' [-118.082615', u' 33.756093]', u' [-117.91485', u' 33.756093]', u' [-117.91485', u' 33.628991]]]}\"'], city=u'Huntington Beach', country=u'United States', text=[u'In need', u' of back rub'], tid=u'669375138903666692', timestamp=u'1448426444160', uid=u'1611125173')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alltweets_df.head()\n",
    "alltweets_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text=[u'In need', u' of back rub'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alltweets_df.select(\"text\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
